{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676d4f31-933a-40d0-86ea-1a8fb368d76b",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "\n",
    "### 1. Introduction to Python Web Scraping\n",
    "\n",
    "#### 1.1 Definition of a Web Scraper\n",
    "\n",
    "- **Web Scraper**: An automated network program that can access web pages on the World Wide Web according to specific rules, extract information, and store the data locally.\n",
    "\n",
    "#### 1.2 Applications of Web Scrapers\n",
    "\n",
    "- Search engines (e.g., Google, Baidu)\n",
    "- Data analysis (market analysis, competitor analysis, etc.)\n",
    "- Monitoring changes in website content (such as stock prices, news updates)\n",
    "\n",
    "#### 1.3 Basic Networking Knowledge and HTTP Protocol Introduction\n",
    "\n",
    "##### 1.3.1 Network Basics\n",
    "\n",
    "- **World Wide Web (WWW)**: A system comprised of many interconnected web pages accessible via the Internet.\n",
    "- **Internet**: A global network of computers connected to exchange data.\n",
    "\n",
    "##### 1.3.2 IP Addresses and Domain Names\n",
    "\n",
    "- **IP Address**: A unique address for every computer on the Internet, e.g., 192.168.1.1.\n",
    "- **Domain Name**: An easier-to-remember address, such as `www.example.com`, resolved to an IP address via the Domain Name System (DNS).\n",
    "\n",
    "##### 1.3.3 HTTP Protocol\n",
    "\n",
    "- **HTTP (Hypertext Transfer Protocol)**: Defines the format and rules for exchanging information between clients and servers.\n",
    "- **Requests and Responses**: Clients send HTTP requests to servers, which return responses.\n",
    "- **Methods**: Major HTTP methods include GET (request resources), POST (submit data for processing), PUT (replace all current representations of the target resource), DELETE (remove the specified resource), etc.\n",
    "\n",
    "##### 1.3.4 HTTPS Protocol\n",
    "\n",
    "- **HTTPS (Hypertext Transfer Protocol Secure)**: Adds SSL/TLS protocol to HTTP for encrypting communications between clients and servers, ensuring data security.\n",
    "- **SSL/TLS**: Used for encrypting data between web browsers and servers.\n",
    "- **Encryption Process**: Ensures data is not stolen or tampered with during transmission, using keys for encryption and decryption.\n",
    "\n",
    "##### 1.3.5 URL Structure\n",
    "\n",
    "- URL (Uniform Resource Locator)  : An address on the Internet for a resource, including protocol, domain name, port (optional), resource path, and query parameters.\n",
    "\n",
    "  - Example: `https://www.example.com:443/path/to/file?query=value`\n",
    "\n",
    "#### 1.4 Common Web Servers and Client Tools\n",
    "\n",
    "- **Web Servers**: Such as Apache, Nginx, Microsoft IIS.\n",
    "- Client Tools:\n",
    "  - Browsers (Chrome, Firefox)\n",
    "  - Command-line tools (curl, wget)\n",
    "  - **Fiddler**: An HTTP debugging tool that captures both HTTP and HTTPS traffic, allowing users to monitor, modify, and replay inbound and outbound data.\n",
    "  - **Charles**: A proxy server that enables developers to view all HTTP and SSL/HTTPS traffic, including requests and responses, headers, and metadata.\n",
    "\n",
    "#### 1.5 Initial Test Sites\n",
    "\n",
    "- **HTTPBin** ([http://httpbin.org](http://httpbin.org/)): A simple service that receives HTTP requests and echoes back sent information. It supports various request methods such as GET, POST, PUT, DELETE, etc., and can be used to test HTTP headers, response data, and status codes.\n",
    "- **Reqres** ([https://reqres.in](https://reqres.in/)): A lightweight mock REST API that provides various API response simulations including user registration, user information retrieval, data updating, as well as simulation of HTTPS information.\n",
    "\n",
    "### Example  for HTTP Requests and HTTPS\n",
    "\n",
    "#### HTTP Requests:\n",
    "\n",
    "- **Example HTTP Request**:\n",
    "\n",
    "  ```\n",
    "  GET /api/users HTTP/1.1\n",
    "  Host: example.com\n",
    "  User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36\n",
    "  Accept: application/json\n",
    "  Cookie: session_id=abc123; language=en-US\n",
    "  Authorization: Bearer <access_token>\n",
    "  ```\n",
    "\n",
    "- **Request Headers**:\n",
    "\n",
    "  - **Cookie**: Sends saved data back to the server.\n",
    "  - **Session**: Maintained by the server using session cookies which help to personalize user interactions without requiring login credentials for each page visited.\n",
    "\n",
    "#### HTTPS Responses:\n",
    "\n",
    "- Example HTTPS Response:\n",
    "\n",
    "  ```\n",
    "  HTTP/1.1 200 OK\n",
    "  Content-Type: application/json\n",
    "  Server: Apache/2.4.41 (Unix)\n",
    "  Set-Cookie: session_id=def456; Expires=Sat, 14 May 2023 23:59:59 GMT; Secure; HttpOnly\n",
    "  Cache-Control: max-age=3600\n",
    "  Content-Length: 45\n",
    "  \n",
    "  {\n",
    "    \"id\": 123,\n",
    "    \"name\": \"John Doe\",\n",
    "    \"email\": \"johndoe@example.com\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "This expanded content integrates detailed explanations of HTTP requests, including headers, the use of sessions and cookies, and provides examples of both HTTP and HTTPS responses to help students better understand web communications within a web scraping context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c9721-b20a-411f-afa3-4d46add00d53",
   "metadata": {},
   "source": [
    "\n",
    "Understanding GET and POST requests:\n",
    "\n",
    "GET is used to retrieve information, while POST is used to send data to a server.\n",
    "\n",
    "Status codes and their meanings:\n",
    "\n",
    "Each HTTP response comes with a status code. For example, 200 means the request was successful, while 404 means the requested resource was not found.\n",
    "\n",
    "HTTP Request:\n",
    "\n",
    "1. Request line: Specifies the HTTP method (e.g., GET, POST), the target URL, and the HTTP version.\n",
    "2. Headers:\n",
    "   - Host: Specifies the domain name or IP address of the server.\n",
    "   - User-Agent: Identifies the client making the request (e.g., browser or software).\n",
    "   - Accept: Specifies the desired content type for the response (e.g., text/html, application/json).\n",
    "   - Content-Type: Indicates the format of the data included in the request body (e.g., application/json, multipart/form-data).\n",
    "   - Cookie: Contains any previously stored cookies sent by the server.\n",
    "   - Authorization: Provides credentials for accessing protected resources (e.g., API keys, access tokens).\n",
    "   - Other headers: Additional information, such as Accept-Language, Referer, User-Agent, etc.\n",
    "3. Body (optional): Contains the payload or data sent with the request, such as form data or JSON payload.\n",
    "\n",
    "HTTP Response:\n",
    "\n",
    "1. Status line: Specifies the HTTP version, the status code indicating the outcome of the request (e.g., 200 OK, 404 Not Found), and a brief reason phrase.\n",
    "2. Headers:\n",
    "   - Content-Type: Indicates the format of the response content (e.g., text/html, application/json).\n",
    "   - Set-Cookie: Sets a cookie on the client's side for future requests.\n",
    "   - Server: Identifies the software or server handling the request.\n",
    "   - Cache-Control: Controls caching behavior on the client or intermediate proxies.\n",
    "   - Content-Length: Specifies the length of the response body in bytes.\n",
    "   - Other headers: Vary, Expires, Last-Modified, etc., providing additional information about the response.\n",
    "3. Body (optional): Contains the actual content of the response, such as HTML, JSON, or binary data.\n",
    "\n",
    "HTTP response codes and their meanings:\n",
    "\n",
    "- **1xx (Informational)**:\n",
    "  - 100 Continue: The server has received the request headers and the client should proceed to send the request body.\n",
    "  - 101 Switching Protocols: The server is switching protocols according to the request.\n",
    "- **2xx (Success)**:\n",
    "  - 200 OK: Standard response for successful HTTP requests, typically used for GET and POST requests.\n",
    "  - 201 Created: The request has been fulfilled and resulted in the creation of a new resource.\n",
    "  - 204 No Content: The server successfully processed the request but is not returning any content.\n",
    "- **3xx (Redirection)**:\n",
    "  - 301 Moved Permanently: The requested resource has been permanently moved to a new URI.\n",
    "  - 302 Found: The requested resource resides temporarily under a different URI.\n",
    "  - 304 Not Modified: The resource has not been modified since the last request.\n",
    "- **4xx (Client Error)**:\n",
    "  - 400 Bad Request: The server cannot process the request due to a client error in syntax.\n",
    "  - 401 Unauthorized: The request requires user authentication.\n",
    "  - 403 Forbidden: The server understood the request but refuses to authorize it.\n",
    "  - 404 Not Found: The server cannot find the requested resource.\n",
    "- **5xx (Server Error)**:\n",
    "  - 500 Internal Server Error: A generic error message, given when an unexpected condition was encountered and no more specific message is suitable.\n",
    "  - 502 Bad Gateway: The server, while acting as a gateway or proxy, received an invalid response from an inbound server it accessed while attempting to fulfill the request.\n",
    "  - 503 Service Unavailable: The server is currently unable to handle the request due to a temporary overload or maintenance.\n",
    "  - 504 Gateway Timeout: The server, while acting as a gateway or proxy, did not receive a timely response from the upstream server it accessed in attempting to complete the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91396b0d-4fc1-47d7-b2c0-318b7a59c937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在 Ping www.a.shifen.com [110.242.68.4] 具有 32 字节的数据:\n",
      "来自 110.242.68.4 的回复: 字节=32 时间=23ms TTL=53\n",
      "来自 110.242.68.4 的回复: 字节=32 时间=19ms TTL=53\n",
      "来自 110.242.68.4 的回复: 字节=32 时间=20ms TTL=53\n",
      "来自 110.242.68.4 的回复: 字节=32 时间=20ms TTL=53\n",
      "\n",
      "110.242.68.4 的 Ping 统计信息:\n",
      "    数据包: 已发送 = 4，已接收 = 4，丢失 = 0 (0% 丢失)，\n",
      "往返行程的估计时间(以毫秒为单位):\n",
      "    最短 = 19ms，最长 = 23ms，平均 = 20ms\n"
     ]
    }
   ],
   "source": [
    "!ping www.baidu.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81c8d4-03c6-4f7f-825a-3e756be0444e",
   "metadata": {},
   "source": [
    "### 2. Legal and Ethical Issues of Python Web Scraping\n",
    "\n",
    "#### 2.1 Understanding the robots.txt File\n",
    "\n",
    "- **Purpose of robots.txt**: This file is used by websites to communicate with web crawlers and tell them where they are allowed or disallowed from crawling. It’s located at the root of the website .\n",
    "- **Content Structure**: The file contains `User-agent` lines specifying different web crawlers, followed by `Disallow` or `Allow` directives to restrict or grant access to specific paths of the website.\n",
    "- **Respecting robots.txt**: Ethical web scraping involves adhering to the restrictions specified in the `robots.txt` file. Ignoring this can lead to legal actions and being banned from websites.\n",
    "\n",
    "#### 2.2 Legal Guidelines for Using Web Scrapers\n",
    "\n",
    "- **Compliance with Laws**: The legality of web scraping depends on the jurisdiction and the specific laws of the country. In general, accessing publicly available data is often legal, but scraping data without permission from protected areas or in violation of terms of service can lead to legal consequences.\n",
    "- **Terms of Service (ToS)**: Many websites include clauses in their ToS that restrict or prohibit scraping. It’s important to review and comply with these terms before scraping data.\n",
    "- **Avoiding System Overload**: Legal issues can also arise from overloading a website’s server by sending too many requests in a short period. This can be considered a denial-of-service attack.\n",
    "\n",
    "#### 2.3 Ethical Considerations in Data Use\n",
    "\n",
    "- **Privacy Concerns**: When scraping data, it's crucial to consider the privacy of individuals. Personal data should be handled with care, and it’s best to anonymize data when possible.\n",
    "- **Data Use**: Ethically, the data collected through scraping should be used responsibly. Misusing data can lead to ethical breaches and damage to individuals or organizations.\n",
    "- **Transparency and Consent**: Whenever possible, obtaining consent for data use and being transparent about how the data will be used can help mitigate ethical risks.\n",
    "\n",
    "#### 2.4 Case Studies and Examples\n",
    "\n",
    "- **Positive Example**: Academic researchers scraping data for analyzing market trends, where they comply with robots.txt, use data ethically, and publish their findings for public benefit.\n",
    "- **Negative Example**: A business scraping contact information from a competitor’s website without consent and using it for spam marketing campaigns, violating privacy and legal guidelines.\n",
    "\n",
    "By adhering to these legal and ethical guidelines, Python web scrapers can ensure their activities are not only effective but also respect the rights and regulations of the online environment. This section of the course could involve discussing real-world cases to illustrate the implications of ethical and legal considerations in web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2453cf-d2f0-47dd-8b69-ad3acc06fb9f",
   "metadata": {},
   "source": [
    "### 3. Basic Components of Python Web Scrapers\n",
    "\n",
    "#### 3.1 Request Library: Requests\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "Requests is a popular Python HTTP library designed to make HTTP requests simple and intuitive. It's built with the philosophy of \"being for humans\", supporting features like session objects, persistent connections, and persistent cookies.\n",
    "\n",
    "##### Installation\n",
    "\n",
    "To install the Requests library, enter the following command in the command line or terminal:\n",
    "\n",
    "```\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "##### Basic Usage\n",
    "\n",
    "Using Requests to send HTTP requests is straightforward. Here are some basic examples:\n",
    "\n",
    "###### Sending GET Requests\n",
    "\n",
    "GET requests are used to retrieve data from a specified URL. The following example shows how to send a GET request and print the response content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ae0586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\users\\fan\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\fan\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\users\\fan\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\users\\fan\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\fan\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f688df5-e1fd-4781-b497-ca4be65d570b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.28.1\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-662f4726-39e30f5464c628db5c0e3204\"\n",
      "  }, \n",
      "  \"origin\": \"122.206.190.72\", \n",
      "  \"url\": \"https://httpbin.org/get\"\n",
      "}\n",
      "\n",
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get('https://httpbin.org/get')\n",
    "print(response.text)  # Print the text content of the response\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13ceda-3dc3-4724-9aef-b48edd8ed8e0",
   "metadata": {},
   "source": [
    "###### Sending POST Requests\n",
    "\n",
    "POST requests are commonly used to send data to the server. Here is how to send a POST request and handle a JSON response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cea4a89-f673-4431-ad16-31eec74d4870",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'args': {}, 'data': '', 'files': {}, 'form': {'key': 'value'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br', 'Content-Length': '9', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.28.1', 'X-Amzn-Trace-Id': 'Root=1-662f47a6-793ff5536cec543b5f41542f'}, 'json': None, 'origin': '122.206.190.72', 'url': 'https://httpbin.org/post'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Send a POST request\n",
    "response = requests.post('https://httpbin.org/post', data={'key': 'value'})\n",
    "print(response)\n",
    "print(response.json())  # Print the JSON content of the response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb8400-19ff-4a90-84f3-c4e598588b16",
   "metadata": {},
   "source": [
    "##### Handling Query Parameters\n",
    "\n",
    "When sending GET requests, you often need to include query parameters in the URL. Requests allow you to provide these parameters as a dictionary, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b08fe77c-f5d5-4bcc-a1ec-45b59225cc02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://httpbin.org/get?key1=value1&key2=value2\n",
      "{\n",
      "  \"args\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.31.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-662e77a6-23ad6b8727d27d7a5412860e\"\n",
      "  }, \n",
      "  \"origin\": \"221.15.159.208\", \n",
      "  \"url\": \"https://httpbin.org/get?key1=value1&key2=value2\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define query parameters\n",
    "params = {\n",
    "    'key1': 'value1',\n",
    "    'key2': 'value2'\n",
    "}\n",
    "\n",
    "# Send the request\n",
    "response = requests.get('https://httpbin.org/get', params=params)\n",
    "print(response.url)  # View the actual URL requested\n",
    "print(response.text)  # Print the text content of the response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f9889-3e1c-403b-b202-2bdf69d9e949",
   "metadata": {},
   "source": [
    "Handling Request Headers\n",
    "If you need to customize HTTP headers, such as setting a User-Agent or an authentication token, you can pass a dictionary to the headers parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce39808c-174f-407b-a486-ff01f56c7b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [412]>\n",
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n",
      "<html lang=\"zh-cn\">\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"Access-Control-Allow-Origin\" content=\"*\" />\n",
      "    <meta http-equiv=\"Page-Enter\" content=\"blendTrans(Duration=0.5)\">\n",
      "    <meta http-equiv=\"Page-Exit\" content=\"blendTrans(Duration=0.5)\">\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\">\n",
      "    <meta name=\"spm_prefix\" content=\"333.937\">\n",
      "    <title>åºéå¦! - bilibili.com</title>\n",
      "    <link rel=\"shortcut icon\" href=\"//static.hdslb.com/images/favicon.ico\">\n",
      "    <script type=\"text/javascript\" src=\"//s1.hdslb.com/bfs/static/jinkela/long/js/jquery/jquery1.7.2.min.js\"></script>\n",
      "    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "    <div class=\"error-container\">\n",
      "        <div class=\"txt-item err-code\">éè¯¯å·:412</div>\n",
      "        <div class=\"txt-item err-text\">ç±äºè§¦ååå©åå©å®å",
      "¨é£æ§ç­ç¥ï¼è¯¥æ¬¡è®¿é®è¯·æ±è¢«æç»ã</div>\n",
      "        <div class=\"txt-item\">The request was rejected because of the bilibili security control policy.</div>\n",
      "        <div class=\"txt-item datetime_now\"></div>\n",
      "        <div class=\"txt-item user_url\"></div>\n",
      "        <div class=\"txt-item user_ip\"></div>\n",
      "        <div class=\"txt-item user_id\"></div>\n",
      "        <div class=\"check-input\">\n",
      "            <div class=\"title\"></div>\n",
      "            <div class=\"box-pic\"></div>\n",
      "            <div class=\"box\"></div>\n",
      "            <div class=\"state\"></div>\n",
      "        </div>\n",
      "    </div>\n",
      "    <script type=\"text/javascript\" charset=\"utf-8\" src=\"//security.bilibili.com/static/js/sha256.min.js\"></script>\n",
      "    <script type=\"text/javascript\" charset=\"utf-8\" src=\"//security.bilibili.com/static/js/js.cookie.min.js\"></script>\n",
      "    <script type=\"text/javascript\" charset=\"utf-8\" src=\"//security.bilibili.com/static/js/412.js\"></script>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of bilibili\n",
    "url = 'https://www.bilibili.com/'\n",
    "\n",
    "# Send a GET request to bilibili\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c15e7-aaf3-40ab-9b3f-bb443bd21181",
   "metadata": {},
   "source": [
    "##### Handling Request Headers\n",
    "\n",
    "If you need to customize HTTP headers, such as setting a `User-Agent` or an authentication token, you can pass a dictionary to the `headers` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b04dc28-d8cb-4c07-8486-d0f094457519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The HTML content has been saved to 'bilili_page.html'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of bilibili\n",
    "url = 'https://www.bilibili.com/'\n",
    "\n",
    "# Define a dictionary containing the headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send a GET request to Baidu with the specified headers\n",
    "response = requests.get(url, headers=headers)\n",
    "# print(response.text)\n",
    "with open('bilili_page.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"\\nThe HTML content has been saved to 'bilili_page.html'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a82247-6e2b-43fc-9896-5e15f4ffa197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET Response: {'page': 1, 'per_page': 6, 'total': 12, 'total_pages': 2, 'data': [{'id': 1, 'email': 'george.bluth@reqres.in', 'first_name': 'George', 'last_name': 'Bluth', 'avatar': 'https://reqres.in/img/faces/1-image.jpg'}, {'id': 2, 'email': 'janet.weaver@reqres.in', 'first_name': 'Janet', 'last_name': 'Weaver', 'avatar': 'https://reqres.in/img/faces/2-image.jpg'}, {'id': 3, 'email': 'emma.wong@reqres.in', 'first_name': 'Emma', 'last_name': 'Wong', 'avatar': 'https://reqres.in/img/faces/3-image.jpg'}, {'id': 4, 'email': 'eve.holt@reqres.in', 'first_name': 'Eve', 'last_name': 'Holt', 'avatar': 'https://reqres.in/img/faces/4-image.jpg'}, {'id': 5, 'email': 'charles.morris@reqres.in', 'first_name': 'Charles', 'last_name': 'Morris', 'avatar': 'https://reqres.in/img/faces/5-image.jpg'}, {'id': 6, 'email': 'tracey.ramos@reqres.in', 'first_name': 'Tracey', 'last_name': 'Ramos', 'avatar': 'https://reqres.in/img/faces/6-image.jpg'}], 'support': {'url': 'https://reqres.in/#support-heading', 'text': 'To keep ReqRes free, contributions towards server costs are appreciated!'}}\n",
      "\n",
      "\n",
      "POST Response: {'name': 'morpheus', 'job': 'leader', 'id': '212', 'createdAt': '2024-04-29T07:43:35.251Z'}\n",
      "\n",
      "\n",
      "PUT Response: {'name': 'morpheus', 'job': 'zion resident', 'updatedAt': '2024-04-29T07:43:36.774Z'}\n",
      "\n",
      "\n",
      "DELETE Response: 204\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 测试 GET 请求\n",
    "def test_get():\n",
    "    response = requests.get('https://reqres.in/api/users?page=1')\n",
    "    print(\"GET Response:\", response.json())\n",
    "\n",
    "# 测试 POST 请求\n",
    "def test_post():\n",
    "    data = {\n",
    "        \"name\": \"morpheus\",\n",
    "        \"job\": \"leader\"\n",
    "    }\n",
    "    response = requests.post('https://reqres.in/api/users', data=data)\n",
    "    print(\"POST Response:\", response.json())\n",
    "\n",
    "# 测试 PUT 请求\n",
    "def test_put():\n",
    "    data = {\n",
    "        \"name\": \"morpheus\",\n",
    "        \"job\": \"zion resident\"\n",
    "    }\n",
    "    response = requests.put('https://reqres.in/api/users/2', data=data)\n",
    "    print(\"PUT Response:\", response.json())\n",
    "\n",
    "# 测试 DELETE 请求\n",
    "def test_delete():\n",
    "    response = requests.delete('https://reqres.in/api/users/2')\n",
    "    print(\"DELETE Response:\", response.status_code)  # 成功删除通常返回 204\n",
    "\n",
    "# 执行测试\n",
    "test_get()\n",
    "print('\\n')\n",
    "\n",
    "test_post()\n",
    "print('\\n')\n",
    "test_put()\n",
    "print('\\n')\n",
    "test_delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c91d6e21-bac5-45ac-ac3d-f17d0ef9b7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing registration success:\n",
      "({'id': 4, 'token': 'QpwL5tke4Pnpja7X4'}, 200)\n",
      "\n",
      "Testing registration failure (missing password):\n",
      "({'error': 'Missing password'}, 400)\n",
      "\n",
      "Testing login success:\n",
      "({'token': 'QpwL5tke4Pnpja7X4'}, 200)\n",
      "\n",
      "Testing login failure (wrong password):\n",
      "({'token': 'QpwL5tke4Pnpja7X4'}, 200)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API base URL\n",
    "base_url = \"https://reqres.in/api\"\n",
    "\n",
    "def register_user(email, password):\n",
    "    \"\"\"Function to register a user.\"\"\"\n",
    "    url = f\"{base_url}/register\"\n",
    "    data = {\n",
    "        \"email\": email,\n",
    "        \"password\": password\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json(), response.status_code\n",
    "\n",
    "def login_user(email, password):\n",
    "    \"\"\"Function to login a user.\"\"\"\n",
    "    url = f\"{base_url}/login\"\n",
    "    data = {\n",
    "        \"email\": email,\n",
    "        \"password\": password\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json(), response.status_code\n",
    "\n",
    "# Test registration success\n",
    "print(\"Testing registration success:\")\n",
    "print(register_user(\"eve.holt@reqres.in\", \"pistol\"))\n",
    "\n",
    "# Test registration failure (missing password)\n",
    "print(\"\\nTesting registration failure (missing password):\")\n",
    "print(register_user(\"eve.holt@reqres.in\", \"\"))\n",
    "\n",
    "# Test login success\n",
    "print(\"\\nTesting login success:\")\n",
    "print(login_user(\"eve.holt@reqres.in\", \"cityslicka\"))\n",
    "\n",
    "# Test login failure (wrong password)\n",
    "print(\"\\nTesting login failure (wrong password):\")\n",
    "print(login_user(\"eve.holt@reqres.in\", \"wrongpassword\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96acd6d1-f811-4f8e-9fc5-b49b927be40d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In web development, `session` and `cookie` are technologies used for storing information, primarily for maintaining the state of users between the browser and the server. While they serve similar purposes, they operate differently and are used for distinct reasons.\n",
    "\n",
    "### Cookie\n",
    "\n",
    "A cookie is a small piece of data sent from a server and stored on the user's browser. Whenever the same user makes a request to the server again, the browser sends the cookie back to the server along with the request. This way, the server can recognize the user and remember information about them, such as their login status, preferences, etc.\n",
    "\n",
    "**Key features include**:\n",
    "\n",
    "- **Persistence**: Cookies can be set with an expiration date. If an expiration date is set, the information remains saved even after the browser is closed; if not set, it becomes a session cookie, which expires when the browser is closed.\n",
    "- **Limited size**: Each cookie is limited to about 4KB, and there is a limit to the number of cookies stored per domain.\n",
    "- **Security**: Although cookie data is stored locally and can be accessed and modified by users, security can be enhanced by setting HttpOnly and Secure flags to prevent cross-site scripting (XSS) attacks from reading cookies or sending cookies over non-encrypted connections.\n",
    "\n",
    "### Session\n",
    "\n",
    "Session is another server-side data storage mechanism used to store information about a user's session. The server assigns a unique identifier, usually called a session ID, to each user's session. This identifier is stored in a cookie or passed through URL rewriting. Each time the user interacts with the server, the server can recognize the user by the session ID and access the data stored on the server about that user.\n",
    "\n",
    "**Key features include**:\n",
    "\n",
    "- **Increased security**: Since session data is stored on the server side, it cannot be accessed directly by users, making it more secure than cookies.\n",
    "- **No size limit**: Sessions can store a larger amount of data without the size limitations of cookies.\n",
    "- **Dependent on cookies**: Although session information is stored on the server, the session ID is typically managed via cookies. If cookies are disabled by the user, other methods (such as URL rewriting) need to be used to pass the session ID.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, cookies are a way to store data in the user's browser, mainly used for tracking and identifying users. Sessions are a server-side solution that provides a method to store user-specific data, managed through session IDs to recognize and manage the user's state. In web applications, both are often used together to implement user authentication, state management, and other functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18eb2b86-04a8-48ea-81e1-011a6c3334ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Cookie\": \"sessioncookie=12345\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.31.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-662e82fe-412039c85d09c5491dfedd0b\"\n",
      "  }, \n",
      "  \"origin\": \"221.15.159.208\", \n",
      "  \"url\": \"https://httpbin.org/get\"\n",
      "}\n",
      "\n",
      "{'sessioncookie': '12345'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 创建一个 Session 对象\n",
    "session = requests.Session()\n",
    "\n",
    "# 添加一个名为 'sessioncookie' 的 cookie 到 session 中\n",
    "session.cookies.set('sessioncookie', '12345')\n",
    "\n",
    "# 发送 GET 请求\n",
    "response = session.get('https://httpbin.org/get')\n",
    "\n",
    "# 打印响应文本，可以看到请求中包含的 cookie\n",
    "print(response.text)\n",
    "\n",
    "# 打印当前 session 中的所有 cookies\n",
    "print(session.cookies.get_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "048ed1e8-8035-45ee-8986-61879415b036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the content you want to search: nih \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "kw = input(\"Please enter the content you want to search: \")\n",
    "response = requests.get(f\"https://www.sogou.com/web?query={kw}\")  # Send GET request\n",
    "\n",
    "with open(\"search_sogou.html\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4378477-0a8b-44ca-a8a8-fd25f2db6a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the content you want to search: hello\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 用户输入搜索内容\n",
    "kw = input(\"Please enter the content you want to search: \")\n",
    "\n",
    "# 创建自定义请求头部\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5'\n",
    "}\n",
    "\n",
    "# 发送带有自定义头部的 GET 请求\n",
    "response = requests.get(f\"https://www.sogou.com/web?query={kw}\", headers=headers)\n",
    "\n",
    "# 将响应内容写入文件\n",
    "with open(\"search_sogou1.html\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb170d32-9b48-41eb-9e9c-765184e05751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the text to translate: apple\n",
      "{'errno': 0, 'data': [{'k': 'Apple', 'v': 'n. 苹果公司，原称苹果电脑公司'}, {'k': 'apple', 'v': 'n. 苹果; 苹果公司; 苹果树'}, {'k': 'APPLE', 'v': 'n. 苹果'}, {'k': 'apples', 'v': 'n. 苹果，苹果树( apple的名词复数 ); [美国口语]棒球; [美国英语][保龄球]坏球; '}, {'k': 'Apples', 'v': '[地名] [瑞士] 阿普勒'}], 'logid': 1192332965}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Request URL\n",
    "url = 'https://fanyi.baidu.com/sug'\n",
    "\n",
    "# Prompt the user to enter the text to translate\n",
    "text = input(\"Please enter the text to translate: \")\n",
    "\n",
    "# Build the request data\n",
    "data = {\n",
    "    'kw': text,    # Text to translate\n",
    "    'from': 'auto',   # Source language automatically detected\n",
    "    'to': 'zh'      # Target language is Chinese\n",
    "}\n",
    "\n",
    "# Send a POST request\n",
    "response = requests.post(url, data=data)\n",
    "\n",
    "# Get the response result\n",
    "result = response.json()\n",
    "\n",
    "# Print the translation result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc86886-3c02-4c96-902d-9fa1e17e9819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the text to translate:apple\n",
      "n. 苹果公司，原称苹果电脑公司\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Prompt the user to enter the text to translate\n",
    "kw = input(\"Please enter the text to translate:\")\n",
    "\n",
    "# Prepare the request data\n",
    "dic = {\n",
    "    \"kw\": kw   # This must match the parameter in the request tool\n",
    "}\n",
    "\n",
    "# Send a POST request to Baidu Translate's 'sug' endpoint\n",
    "resp = requests.post(\"https://fanyi.baidu.com/sug\", data=dic)\n",
    "\n",
    "# The response is JSON, so parse it directly\n",
    "resp_json = resp.json()\n",
    "\n",
    "# Extract the translation from the response\n",
    "# Here we're assuming the translation we want is always in the first dictionary in the 'data' list\n",
    "# If this is not the case, you might need to modify this part\n",
    "print(resp_json['data'][0]['v'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "021afefa-5f5d-4026-b02d-750c37f296ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the start position (from which movie to start): 1\n",
      "Enter the number of movies to fetch: 10\n",
      "[{'rating': ['9.3', '50'], 'rank': 2, 'cover_url': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2553104888.jpg', 'is_playable': False, 'id': '1291858', 'types': ['剧情', '喜剧'], 'regions': ['中国大陆'], 'title': '鬼子来了', 'url': 'https://movie.douban.com/subject/1291858/', 'release_date': '2000-05-12', 'actor_count': 30, 'vote_count': 655098, 'score': '9.3', 'actors': ['姜文', '香川照之', '袁丁', '姜宏波', '丛志军', '李丛喜', '泽田谦也', '李海滨', '蔡卫东', '陈述', '陈莲梅', '史建全', '陈强', '宫路佳具', '吴大维', '梶冈润一', '石山雄大', '述平', '姜武', '姜金才', '石山雄太', '山田将之', '贾幼然', '王义和', '杜世儒', '周海超', '白云生', '徐海东', '长野客弘', '鱼见亮介'], 'is_watched': False}, {'rating': ['9.3', '50'], 'rank': 3, 'cover_url': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p1454261925.jpg', 'is_playable': True, 'id': '6786002', 'types': ['剧情', '喜剧'], 'regions': ['法国'], 'title': '触不可及', 'url': 'https://movie.douban.com/subject/6786002/', 'release_date': '2011-11-02', 'actor_count': 22, 'vote_count': 1174145, 'score': '9.3', 'actors': ['弗朗索瓦·克鲁塞', '奥玛·希', '安娜·勒尼', '奥德雷·弗勒罗', '约瑟芬娜·德·摩', '克洛蒂尔德·莫莱特', '阿尔芭·贝露琪', '萨丽马特·卡马特', '托马·索利韦尔', '卡罗琳·伯格', '玛丽-洛尔·德库洛', '安托万·劳伦特', '弗朗索瓦·卡隆', '多米尼克·达吉尔', '本杰明·巴奇', '尼基·马伯特', '让-弗朗索瓦·凯雷', '艾米丽·卡恩', '多萝特博里埃', '皮埃尔-罗兰·巴纳隆', '海迪·布奇纳法', '弗朗索瓦·伯尔卢普'], 'is_watched': False}, {'rating': ['9.3', '50'], 'rank': 4, 'cover_url': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2263408369.jpg', 'is_playable': True, 'id': '1294371', 'types': ['剧情', '喜剧', '爱情'], 'regions': ['美国'], 'title': '摩登时代', 'url': 'https://movie.douban.com/subject/1294371/', 'release_date': '1936-02-25', 'actor_count': 37, 'vote_count': 316530, 'score': '9.3', 'actors': ['查理·卓别林', '宝莲·高黛', '亨利·伯格曼', '蒂尼·桑福德', '切斯特·康克林', '汉克·曼', '斯坦利·布莱斯通', '阿尔·欧内斯特·加西亚', '理查德·亚历山大', '塞西尔·雷诺兹', '米拉·麦金尼', '默多克·麦夸里', '威尔弗雷德·卢卡斯', '爱德华·勒桑', '弗雷德·马拉泰斯塔', '萨米·斯坦', '特德·奥利弗', '诺曼·安斯利', '博比·巴伯', '海尼·康克林', '格洛丽亚·德黑文', '帕特·弗莱厄蒂', '弗兰克·哈格尼', '帕特·哈蒙', '劳埃德·英格拉哈姆', '沃尔特·詹姆斯', '爱德华·金博尔', '杰克·洛', '巴迪·梅辛杰', '布鲁斯·米切尔', '弗兰克·莫兰', '詹姆斯·C·莫顿', '路易·纳托', 'J·C·纽金特', '拉斯·鲍威尔', '约翰兰德', '哈里·威尔逊'], 'is_watched': False}, {'rating': ['9.2', '45'], 'rank': 5, 'cover_url': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2455050536.jpg', 'is_playable': True, 'id': '1292213', 'types': ['喜剧', '爱情', '奇幻', '古装'], 'regions': ['中国香港', '中国大陆'], 'title': '大话西游之大圣娶亲', 'url': 'https://movie.douban.com/subject/1292213/', 'release_date': '2014-10-24', 'actor_count': 17, 'vote_count': 1590135, 'score': '9.2', 'actors': ['周星驰', '吴孟达', '朱茵', '蔡少芬', '蓝洁瑛', '莫文蔚', '罗家英', '刘镇伟', '陆树铭', '李健仁', '吴珏瑾', '江约诚', '胡立成', '许敬义', '付博文', '侯艳', '金永钢'], 'is_watched': False}, {'rating': ['9.2', '50'], 'rank': 6, 'cover_url': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2614500649.jpg', 'is_playable': True, 'id': '25662329', 'types': ['喜剧', '动画', '冒险'], 'regions': ['美国'], 'title': '疯狂动物城', 'url': 'https://movie.douban.com/subject/25662329/', 'release_date': '2016-03-04', 'actor_count': 43, 'vote_count': 2039768, 'score': '9.2', 'actors': ['金妮弗·古德温', '杰森·贝特曼', '伊德里斯·艾尔巴', '珍妮·斯蕾特', '内特·托伦斯', '邦尼·亨特', '唐·雷克', '汤米·钟', 'J·K·西蒙斯', '奥克塔维亚·斯宾瑟', '艾伦·图代克', '夏奇拉', '雷蒙德·S·佩尔西', '德拉·萨巴', '莫里斯·拉马奇', '菲尔·约翰斯顿', '约翰·迪·马吉欧', '凯蒂·洛斯', '吉塔·雷迪', '杰西·科尔蒂', '汤米·利斯特', '乔希·达拉斯', '瑞奇·摩尔', '凯斯·索西', '彼得·曼斯布里奇', '拜伦·霍华德', '杰拉德·布什', '马克·史密斯', '乔西·特立尼达', '约翰·拉维尔', '克里斯汀·贝尔', '吉尔·科德斯', '梅利莎·古德温', '黄子华', '蔡依林', '容祖儿', '季冠霖', '戴维德·迪格斯', '佟心竹', '张震', '尼古拉斯·格斯特', '李楠', '四刀辉彰'], 'is_watched': False}, {'rating': ['9.2', '50'], 'rank': 7, 'cover_url': 'https://img2.doubanio.com/view/photo/s_ratio_poster/public/p579729551.jpg', 'is_playable': True, 'id': '3793023', 'types': ['剧情', '喜剧', '爱情', '歌舞'], 'regions': ['印度'], 'title': '三傻大闹宝莱坞', 'url': 'https://movie.douban.com/subject/3793023/', 'release_date': '2011-12-08', 'actor_count': 13, 'vote_count': 1928294, 'score': '9.2', 'actors': ['阿米尔·汗', '卡琳娜·卡普尔', '马达范', '沙尔曼·乔希', '奥米·瓦依达', '博曼·伊拉尼', '莫娜·辛格', '拉杰夫·拉宾德拉纳特安', 'Atul Tiwari', '阿里·法扎勒', '帕里卡沙特.萨赫尼', 'Rakesh Sharma', '贾维德·杰弗里'], 'is_watched': False}, {'rating': ['9.3', '50'], 'rank': 8, 'cover_url': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2170238828.jpg', 'is_playable': True, 'id': '1293908', 'types': ['喜剧', '剧情', '爱情'], 'regions': ['美国'], 'title': '城市之光', 'url': 'https://movie.douban.com/subject/1293908/', 'release_date': '1931-01-30', 'actor_count': 13, 'vote_count': 155405, 'score': '9.3', 'actors': ['查理·卓别林', '弗吉尼亚·切瑞尔', '佛罗伦斯·李', '亨利·伯格曼', '珍·哈露', '哈里·克罗克', '阿尔·欧内斯特·加西亚', '汉克·曼', '哈利·迈耶斯', '约翰兰德', '罗伯特·帕里什', '罗伯特·格雷夫斯', '艾伯特·奥斯汀'], 'is_watched': False}, {'rating': ['9.1', '45'], 'rank': 9, 'cover_url': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p501177648.jpg', 'is_playable': True, 'id': '3319755', 'types': ['剧情', '喜剧', '爱情'], 'regions': ['美国'], 'title': '怦然心动', 'url': 'https://movie.douban.com/subject/3319755/', 'release_date': '2010-07-26', 'actor_count': 32, 'vote_count': 1903996, 'score': '9.1', 'actors': ['玛德琳·卡罗尔', '卡兰·麦克奥利菲', '瑞贝卡·德·莫妮', '安东尼·爱德华兹', '约翰·马奥尼', '佩内洛普·安·米勒', '艾丹·奎因', '凯文·韦斯曼', '摩根·莉莉', '瑞安·克茨纳', '吉莉安·普法夫', '迈克尔·博萨', '博·勒纳', '杰奎琳·埃沃拉', '泰勒·格鲁秀斯', '艾莉·布莱恩特', '阿什莉·泰勒', '伊瑟尔·布罗萨德', '科迪·霍恩', '迈克尔·博尔顿', '肖恩·哈珀', '斯戴芬妮·斯考特', '帕特丽夏·伦茨', '马修·戈尔德', '阿罗拉·凯瑟琳·史密斯', '凯莉·唐纳利', '索菲亚·撒高', '米歇尔·梅斯默', '斯科特·乔尔·吉兹基', '罗德·迈尔斯', '卡拉 帕西托', '凯特琳·帕西托'], 'is_watched': False}, {'rating': ['9.1', '45'], 'rank': 10, 'cover_url': 'https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2505426431.jpg', 'is_playable': True, 'id': '20495023', 'types': ['喜剧', '动画', '奇幻', '音乐'], 'regions': ['美国'], 'title': '寻梦环游记', 'url': 'https://movie.douban.com/subject/20495023/', 'release_date': '2017-11-24', 'actor_count': 25, 'vote_count': 1771402, 'score': '9.1', 'actors': ['安东尼·冈萨雷斯', '盖尔·加西亚·贝纳尔', '本杰明·布拉特', '阿兰娜·乌巴赫', '芮妮·维克托', '杰米·卡米尔', '阿方索·阿雷奥', '赫伯特·西古恩萨', '加布里埃尔·伊格莱西亚斯', '隆巴多·博伊尔', '安娜·奥菲丽亚·莫吉亚', '娜塔丽·科尔多瓦', '赛琳娜·露娜', '爱德华·詹姆斯·奥莫斯', '索菲亚·伊斯皮诺萨', '卡拉·梅迪纳', '黛娅娜·欧特里', '路易斯·瓦尔德斯', '布兰卡·阿拉切利', '萨尔瓦多·雷耶斯', '切奇·马林', '奥克塔维·索利斯', '约翰·拉森贝格', '以利亚·罗德里格斯', '石桥阳彩'], 'is_watched': False}, {'rating': ['9.1', '45'], 'rank': 11, 'cover_url': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2553594918.jpg', 'is_playable': True, 'id': '2129039', 'types': ['剧情', '喜剧', '动画', '冒险'], 'regions': ['美国'], 'title': '飞屋环游记', 'url': 'https://movie.douban.com/subject/2129039/', 'release_date': '2009-08-04', 'actor_count': 21, 'vote_count': 1382232, 'score': '9.1', 'actors': ['爱德华·阿斯纳', '克里斯托弗·普卢默', '乔丹·长井', '鲍勃·彼德森', '德尔罗伊·林多', '杰罗姆·兰福特', '约翰·拉森贝格', '大卫·卡耶', '艾丽·道克特', '杰里米·利里', '米凯·麦高万', '丹尼·曼恩', '唐纳德·富利洛夫', '杰斯·哈梅尔', '乔什·库雷', '彼特·道格特', '布兰达∙查普曼', '泰迪·牛顿', '保罗·伊丁', '托尼·弗希勒', '雪莉·琳恩'], 'is_watched': False}]\n",
      "Successfully fetched the movie data!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_douban_movies():\n",
    "    url = 'https://movie.douban.com/j/chart/top_list'\n",
    "\n",
    "    # Get user input for the start position and limit\n",
    "    start = input(\"Enter the start position (from which movie to start): \")\n",
    "    limit = input(\"Enter the number of movies to fetch: \")\n",
    "\n",
    "    param = {\n",
    "        'type': '24',\n",
    "        'interval_id': '100:90',\n",
    "        'action':'',\n",
    "        'start': start,  # From which movie to fetch\n",
    "        'limit': limit,  # Number of movies to fetch\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Make the request and handle any exceptions\n",
    "    try:\n",
    "        response = requests.get(url=url,params=param,headers=headers)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding\n",
    "\n",
    "        # Convert the response to JSON\n",
    "        list_data = response.json()\n",
    "        print(list_data)\n",
    "\n",
    "        # Write the data to a file\n",
    "        with open('./douban.json','w',encoding='utf-8') as fp:\n",
    "            json.dump(list_data, fp, ensure_ascii=False)\n",
    "\n",
    "        print('Successfully fetched the movie data!')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the movie data: {e}\")\n",
    "\n",
    "# Call the function\n",
    "fetch_douban_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc1c104e-863a-4fd9-9f0b-8f5a067d3bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the start position (from which movie to start): 2\n",
      "Enter the number of movies to fetch: 5\n",
      "触不可及\n",
      "摩登时代\n",
      "大话西游之大圣娶亲\n",
      "疯狂动物城\n",
      "三傻大闹宝莱坞\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_douban_movies():\n",
    "    url = 'https://movie.douban.com/j/chart/top_list'\n",
    "\n",
    "    # Get user input for the start position and limit\n",
    "    start = input(\"Enter the start position (from which movie to start): \")\n",
    "    limit = input(\"Enter the number of movies to fetch: \")\n",
    "\n",
    "    param = {\n",
    "        'type': '24',   # This represents the type of the list. '24' stands for \"Top Chinese Movies\" \n",
    "        'interval_id': '100:90',   # This represents the score range. '100:90' means 90-100 score\n",
    "        'action':'',  \n",
    "        'start': start,  # This represents the starting index for the movies to fetch\n",
    "        'limit': limit,  # This represents the number of movies to fetch\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'\n",
    "        # This is just a common user agent string. Some websites require this to allow the request\n",
    "    }\n",
    "\n",
    "    # Make the request and handle any exceptions\n",
    "    try:\n",
    "        response = requests.get(url=url,params=param,headers=headers)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding\n",
    "\n",
    "        # Convert the response to JSON\n",
    "        list_data = response.json()\n",
    "\n",
    "        # Print the movie names\n",
    "        for movie in list_data:\n",
    "            print(movie['title'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the movie data: {e}\")\n",
    "\n",
    "# Call the function\n",
    "fetch_douban_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f02f2b3-60fa-4daf-adf2-7f5132cd6ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Details:\n",
      "URL: https://www.baidu.com/\n",
      "Method: GET\n",
      "Headers: {'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
      "\n",
      "Response Details:\n",
      "Status Code: 200\n",
      "Headers: {'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sun, 28 Apr 2024 16:52:03 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:23:46 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}\n",
      "\n",
      "Response Body:\n",
      "<!DOCTYPE html>\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=百度一下 class=\"bg s_btn\" autofocus></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=https://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">登录</a>');\n",
      "                </script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\n",
      "\n",
      "\n",
      "The HTML content has been saved to 'baidu_page.html'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of Baidu\n",
    "url = 'https://www.baidu.com'\n",
    "\n",
    "# Send a GET request to Baidu\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the response encoding is set correctly\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "# Print the request details\n",
    "print(\"Request Details:\")\n",
    "print(\"URL:\", response.request.url)\n",
    "print(\"Method:\", response.request.method)\n",
    "print(\"Headers:\", response.request.headers)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\nResponse Details:\")\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Headers:\", response.headers)\n",
    "print(\"\\nResponse Body:\")\n",
    "print(response.text[:10000])  # Print the first 10000 characters of the response\n",
    "\n",
    "# Save the response content as an HTML file\n",
    "with open('baidu_page.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"\\nThe HTML content has been saved to 'baidu_page.html'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486dcac-5771-4092-af1d-447ec945959d",
   "metadata": {},
   "source": [
    "### 4. Data Parsing with Python\n",
    "\n",
    "In the previous chapter, we essentially mastered the fundamental skills of scraping an entire webpage. However, in most cases, we don't need the entire webpage content; we only require a small portion of it. So, what can we do? This brings us to the issue of data extraction.\n",
    "\n",
    "Data extraction involves retrieving specific data elements or information from a larger dataset or webpage. Instead of dealing with the entire page, we can use various techniques and tools to extract only the relevant data we need. This allows us to focus on the specific information of interest, making our scraping process more efficient and targeted.\n",
    "\n",
    "There are different methods and approaches to perform data extraction during web scraping. Some common techniques include using BeautifulSoup, Regular Expressions (re), and XPath.\n",
    "\n",
    "BeautifulSoup: It is a Python library that provides a convenient way to parse HTML and XML documents. With BeautifulSoup, we can navigate the HTML structure and extract specific elements or data based on their tags, attributes, or other patterns.\n",
    "\n",
    "Regular Expressions (re): Regular Expressions offer a powerful and flexible approach for pattern matching and text manipulation. Using regex, we can define specific patterns and extract data that matches those patterns from the webpage content.\n",
    "\n",
    "XPath: XPath is a query language used to navigate and select elements in XML or HTML documents. It provides a way to traverse the document structure and select specific nodes or data based on their location or attributes.\n",
    "\n",
    "By employing these techniques, we can efficiently and precisely extract the desired data from webpages, focusing only on the relevant information needed for our analysis or application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681caf41-2279-4d31-853b-814d7352e6c3",
   "metadata": {},
   "source": [
    "#### 4.1 Regular Expressions (re)\n",
    "\n",
    "Step 1: Importing the re Module To use regular expressions in Python, you need to import the `re` module:\n",
    "\n",
    "```python\n",
    "import re\n",
    "```\n",
    "\n",
    "Step 2: Basic Pattern Matching The most basic use of regular expressions is to match a specific pattern in a string. Here's an example:\n",
    "\n",
    "```python\n",
    "pattern = r\"apple\"\n",
    "text = \"I have an apple and a banana.\"\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    print(\"Pattern found!\")\n",
    "else:\n",
    "    print(\"Pattern not found.\")\n",
    "```\n",
    "\n",
    "In this example, we define a pattern using a raw string `r\"apple\"`. We then use `re.search()` to search for that pattern within the `text` string. If the pattern is found, we print \"Pattern found!\"; otherwise, we print \"Pattern not found.\"\n",
    "\n",
    "Step 3: Metacharacters and Special Sequences Regular expressions have special characters called metacharacters that carry special meaning. Here are a few commonly used metacharacters:\n",
    "\n",
    "*   `.`: Matches any character except a newline.\n",
    "*   `^`: Matches the start of a string.\n",
    "*   `$`: Matches the end of a string.\n",
    "*   `[]`: Matches any single character within the brackets.\n",
    "*   `|`: Matches either the expression before or after the pipe.\n",
    "*   `*`: Matches zero or more occurrences of the preceding pattern.\n",
    "*   `+`: Matches one or more occurrences of the preceding pattern.\n",
    "*   `?`: Matches zero or one occurrence of the preceding pattern.\n",
    "*   `()`: Creates a capturing group.\n",
    "\n",
    "Special sequences are shorthand codes that represent common patterns:\n",
    "\n",
    "*   `\\d`: Matches any digit character (0-9).\n",
    "*   `\\w`: Matches any alphanumeric character (a-z, A-Z, 0-9, and underscore).\n",
    "*   `\\s`: Matches any whitespace character (space, tab, newline).\n",
    "*   `\\b`: Matches a word boundary.\n",
    "\n",
    "Step 4: Using Patterns with Functions The `re` module provides various functions for working with regular expressions. Here are some commonly used functions:\n",
    "\n",
    "*   `re.search(pattern, string)`: Searches for a pattern match anywhere in the string.\n",
    "*   `re.match(pattern, string)`: Searches for a pattern match at the beginning of the string.\n",
    "*   `re.findall(pattern, string)`: Returns all non-overlapping matches of the pattern in the string.\n",
    "*   `re.split(pattern, string)`: Splits the string by the occurrences of the pattern.\n",
    "*   `re.sub(pattern, repl, string)`: Replaces occurrences of the pattern in the string with the replacement string.\n",
    "\n",
    "Step 5: Capturing Groups and Backreferences Capturing groups allow you to extract specific parts of a matched pattern. Here's an example:\n",
    "\n",
    "```python\n",
    "pattern = r\"(\\d+)-(\\d+)-(\\d+)\"\n",
    "text = \"Date: 2023-05-14\"\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    year = match.group(1)\n",
    "    month = match.group(2)\n",
    "    day = match.group(3)\n",
    "    print(\"Year:\", year)\n",
    "    print(\"Month:\", month)\n",
    "    print(\"Day:\", day)\n",
    "```\n",
    "\n",
    "In this example, the pattern `(\\d+)-(\\d+)-(\\d+)` captures the year, month, and day from a date string. We use the `match.group()` method to access the captured groups and print them.\n",
    "\n",
    "These are just some of the basics of using regular expressions in Python. Regular expressions offer a powerful way to search, match, and manipulate text patterns in a flexible manner. I recommend referring to the official Python documentation for more detailed information on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce4a2d9-d4fe-4c4b-ad74-8f51a04f5212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"apple\" \n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    print(\"Pattern found!\")\n",
    "else:\n",
    "    print(\"Pattern not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "812aa0a7-49d4-4b94-8b1b-0a4ec2c87c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2023\n",
      "Month: 05\n",
      "Day: 14\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"(\\d+)-(\\d+)-(\\d+)\"\n",
    "text = \"Date: 2023-05-14\"\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    year = match.group(1)\n",
    "    month = match.group(2)\n",
    "    day = match.group(3)\n",
    "    print(\"Year:\", year)\n",
    "    print(\"Month:\", month)\n",
    "    print(\"Day:\", day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95e7ee18-35a0-410d-8b7a-b1cee1cfc9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10086', '10010']\n",
      "10086\n",
      "10010\n",
      "10086\n",
      "10086\n",
      "10086\n",
      "10010\n",
      "['1000000000']\n",
      "Guo Qilin\n",
      "1\n",
      "Song Tie\n",
      "2\n",
      "Da Congming\n",
      "3\n",
      "Fan Sizhe\n",
      "4\n",
      "Hu Shuo Badao\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# findall: Matches all occurrences of the pattern in the string\n",
    "lst = re.findall(r\"\\d+\", \"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "print(lst)\n",
    "\n",
    "# finditer: Matches all occurrences of the pattern in the string [returns an iterator], accessing the content from the iterator requires .group()\n",
    "it = re.finditer(r\"\\d+\", \"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "for i in it:\n",
    "    print(i.group())\n",
    "\n",
    "# search: Returns the first occurrence of a match, the result is a match object, accessing the data requires .group()\n",
    "s = re.search(r\"\\d+\", \"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "print(s.group())\n",
    "\n",
    "# match: Matches from the beginning of the string\n",
    "s = re.match(r\"\\d+\", \"10086, and my girlfriend's phone number is: 10010\")\n",
    "print(s.group())\n",
    "\n",
    "# Precompile regular expression\n",
    "obj = re.compile(r\"\\d+\")\n",
    "\n",
    "ret = obj.finditer(\"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "for it in ret:\n",
    "    print(it.group())\n",
    "\n",
    "ret = obj.findall(\"Hahaha, I don't believe you won't change me 1000000000\")\n",
    "print(ret)\n",
    "\n",
    "s = \"\"\"\n",
    "<div class='jay'><span id='1'>Guo Qilin</span></div>\n",
    "<div class='jj'><span id='2'>Song Tie</span></div>\n",
    "<div class='jolin'><span id='3'>Da Congming</span></div>\n",
    "<div class='sylar'><span id='4'>Fan Sizhe</span></div>\n",
    "<div class='tory'><span id='5'>Hu Shuo Badao</span></div>\n",
    "\"\"\"\n",
    "\n",
    "# (?P<group_name>regex) can be used to further extract content from the matched content\n",
    "obj = re.compile(r\"<div class='.*?'><span id='(?P<id>\\d+)'>(?P<wahaha>.*?)</span></div>\", re.S)  # re.S: allows . to match newline characters\n",
    "\n",
    "result = obj.finditer(s)\n",
    "for it in result:\n",
    "    print(it.group(\"wahaha\"))\n",
    "    print(it.group(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18fdb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10086', '10010']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# findall: Matches all occurrences of the pattern in the string\n",
    "lst = re.findall(r\"\\d+\", \"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c576a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10086\n",
      "10010\n"
     ]
    }
   ],
   "source": [
    "# finditer: Matches all occurrences of the pattern in the string [returns an iterator], accessing the content from the iterator requires .group()\n",
    "it = re.finditer(r\"\\d+\", \"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "for i in it:\n",
    "    print(i.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56701ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10086\n"
     ]
    }
   ],
   "source": [
    "# search: Returns the first occurrence of a match, the result is a match object, accessing the data requires .group()\n",
    "s = re.search(r\"\\d+\", \"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "print(s.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160aef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10086\n",
      "10010\n",
      "['1000000000']\n"
     ]
    }
   ],
   "source": [
    "# Precompile regular expression\n",
    "obj = re.compile(r\"\\d+\")\n",
    "\n",
    "ret = obj.finditer(\"My phone number is: 10086, and my girlfriend's phone number is: 10010\")\n",
    "for it in ret:\n",
    "    print(it.group())\n",
    "\n",
    "ret = obj.findall(\"Hahaha, I don't believe you won't change me 1000000000\")\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecccac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guo Qilin\n",
      "1\n",
      "Song Tie\n",
      "2\n",
      "Da Congming\n",
      "3\n",
      "Fan Sizhe\n",
      "4\n",
      "Hu Shuo Badao\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "<div class='jay'><span id='1'>Guo Qilin</span></div>\n",
    "<div class='jj'><span id='2'>Song Tie</span></div>\n",
    "<div class='jolin'><span id='3'>Da Congming</span></div>\n",
    "<div class='sylar'><span id='4'>Fan Sizhe</span></div>\n",
    "<div class='tory'><span id='5'>Hu Shuo Badao</span></div>\n",
    "\"\"\"\n",
    "\n",
    "# (?P<group_name>regex) can be used to further extract content from the matched content\n",
    "obj = re.compile(r\"<div class='.*?'><span id='(?P<id>\\d+)'>(?P<wahaha>.*?)</span></div>\", re.S)  # re.S: allows . to match newline characters\n",
    "\n",
    "result = obj.finditer(s)\n",
    "for it in result:\n",
    "    print(it.group(\"wahaha\"))\n",
    "    print(it.group(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f8f12d30-287a-4cec-b0b3-d4b6071f3836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and writing to CSV complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "url = \" \"\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36\"\n",
    "}\n",
    "resp = requests.get(url, headers=headers)\n",
    "page_content = resp.text\n",
    "\n",
    "# Parse data\n",
    "pattern = re.compile(r'<li>.*?<div class=\"item\">.*?<span class=\"title\">(?P<name>.*?)'\n",
    "                     r'</span>.*?<p class=\"\">.*?<br>(?P<year>.*?)&nbsp.*?<span '\n",
    "                     r'class=\"rating_num\" property=\"v:average\">(?P<score>.*?)</span>.*?'\n",
    "                     r'<span>(?P<num>.*?)人评价</span>', re.S)\n",
    "\n",
    "\n",
    "# Start matching\n",
    "result = pattern.finditer(page_content)\n",
    "\n",
    "# Create and write to CSV file\n",
    "with open(\"data.csv\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    for item in result:\n",
    "        dic = item.groupdict()\n",
    "        dic['year'] = dic['year'].strip()\n",
    "        csvwriter.writerow(dic.values())\n",
    "\n",
    "print(\"Data extraction and writing to CSV complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4579097-36c8-40c2-bb85-757398fa9af4",
   "metadata": {},
   "source": [
    "1.  `<li>.*?<div class=\"item\">.*?<span class=\"title\">(?P<name>.*?)</span>`\n",
    "\n",
    "    *   `<li>`: Matches the starting `<li>` tag.\n",
    "    *   `.*?`: Matches any characters (except newlines) zero or more times, non-greedily.\n",
    "    *   `<div class=\"item\">`: Matches the `<div>` tag with the class attribute \"item\".\n",
    "    *   `<span class=\"title\">`: Matches the starting `<span>` tag with the class attribute \"title\".\n",
    "    *   `(?P<name>.*?)`: Capturing group named \"name\" to match the movie name. `.*?` matches any characters (except newlines) zero or more times, non-greedily.\n",
    "    *   `</span>`: Matches the closing `</span>` tag.\n",
    "2.  `.*?<p class=\"\">.*?<br>(?P<year>.*?)&nbsp.*?<span class=\"rating_num\" property=\"v:average\">(?P<score>.*?)</span>`\n",
    "\n",
    "    *   `.*?<p class=\"\">`: Matches any characters (except newlines) zero or more times, non-greedily, followed by the `<p>` tag with the class attribute \"\" (empty string).\n",
    "    *   `.*?<br>`: Matches any characters (except newlines) zero or more times, non-greedily, followed by the `<br>` tag.\n",
    "    *   `(?P<year>.*?)`: Capturing group named \"year\" to match the movie year. `.*?` matches any characters (except newlines) zero or more times, non-greedily.\n",
    "    *   `&nbsp`: Matches the non-breaking space character.\n",
    "    *   `.*?<span class=\"rating_num\" property=\"v:average\">`: Matches any characters (except newlines) zero or more times, non-greedily, followed by the `<span>` tag with the class attribute \"rating\\_num\" and property attribute \"v:average\".\n",
    "    *   `(?P<score>.*?)`: Capturing group named \"score\" to match the movie score. `.*?` matches any characters (except newlines) zero or more times, non-greedily.\n",
    "    *   `</span>`: Matches the closing `</span>` tag.\n",
    "3.  `.*?<span>(?P<num>.*?)人评价</span>`\n",
    "\n",
    "    *   `.*?<span>`: Matches any characters (except newlines) zero or more times, non-greedily, followed by the `<span>` tag.\n",
    "    *   `(?P<num>.*?)`: Capturing group named \"num\" to match the number of ratings. `.*?` matches any characters (except newlines) zero or more times, non-greedily.\n",
    "    *   `人评价</span>`: Matches the text \"人评价\" followed by the closing `</span>` tag.\n",
    "\n",
    "The regular expression pattern is designed to match the relevant information for each movie on the Douban top 250 page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf3ec7-e78f-49a7-9c36-8457c2022e86",
   "metadata": {},
   "source": [
    "#### 4.2 BeautifulSoup library\n",
    "\n",
    "*   Parsing HTML using BeautifulSoup\n",
    "*   Navigating parse tree with BeautifulSoup\n",
    "\n",
    "BeautifulSoup (often abbreviated as bs4) is a valuable Python library when it comes to handling web pages or HTML files. It offers a simple and flexible way to parse HTML and extract data from it. Here are some key functionalities of the BeautifulSoup library:\n",
    "\n",
    "1.  HTML Parsing: BeautifulSoup can parse HTML content into a Python object called a \"BeautifulSoup object.\" This object represents the structure of the entire HTML document, allowing you to easily traverse and manipulate it.\n",
    "\n",
    "2.  Navigating the Parse Tree: BeautifulSoup provides a range of methods to navigate the HTML parse tree. You can search for specific elements based on tags, attributes, or hierarchical relationships, or iterate through the entire tree structure to retrieve the desired data.\n",
    "\n",
    "3.  Data Extraction: With BeautifulSoup, you can effortlessly extract data from HTML documents. You can access the content, attributes, and text of individual tags, as well as extract multiple elements based on specific selectors.\n",
    "\n",
    "4.  Modifying Documents: BeautifulSoup also allows you to modify HTML documents. You can add, delete, and modify tags, change tag attributes and text content, and restructure the document as needed.\n",
    "\n",
    "5.  Handling Complex HTML: BeautifulSoup is powerful in handling complex HTML documents. It can handle incomplete tags, nested tag structures, and other HTML errors, ensuring that you can parse and extract data correctly.\n",
    "\n",
    "In summary, BeautifulSoup is a powerful library that is useful for extracting data from HTML, handling web pages, and performing web scraping tasks. It provides a simple and flexible API, making HTML parsing and manipulation easier. Whether it's web scraping, data extraction, or web page analysis, BeautifulSoup is a valuable tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5f5bbefa-34c1-43f9-b30d-79b1a21494be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49c23f3e-44a8-49f8-8dab-3d36f0a11c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Page\n",
      "Welcome to the Example Page\n",
      "<div class=\"content\">\n",
      "<p>This is some example content.</p>\n",
      "<ul>\n",
      "<li>Item 1</li>\n",
      "<li>Item 2</li>\n",
      "<li>Item 3</li>\n",
      "</ul>\n",
      "</div>\n",
      "['content']\n",
      "Item 1\n",
      "Item 2\n",
      "Item 3\n",
      "This is some example content.\n",
      "Item 1\n",
      "Item 2\n",
      "Item 3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Example Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to the Example Page</h1>\n",
    "    <div class=\"content\">\n",
    "      <p>This is some example content.</p>\n",
    "      <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Access tag content\n",
    "title = soup.title\n",
    "print(title.text)  # Output: Example Page\n",
    "\n",
    "h1 = soup.h1\n",
    "print(h1.text)  # Output: Welcome to the Example Page\n",
    "\n",
    "# Find elements by tag name\n",
    "div = soup.find('div')\n",
    "print(div)\n",
    "\n",
    "# Access element attributes\n",
    "div_class = div['class']\n",
    "print(div_class)  # Output: ['content']\n",
    "\n",
    "# Iterate through tag elements\n",
    "ul = soup.find('ul')\n",
    "for li in ul.find_all('li'):\n",
    "    print(li.text)\n",
    "\n",
    "# Select elements using CSS selectors\n",
    "p = soup.select_one('.content p')\n",
    "print(p.text)  # Output: This is some example content.\n",
    "\n",
    "items = soup.select('.content li')\n",
    "for item in items:\n",
    "    print(item.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6d184b0-7726-4cef-a448-e2b373752415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肖申克的救赎\n",
      " / The Shawshank Redemption\n",
      "霸王别姬\n",
      "阿甘正传\n",
      " / Forrest Gump\n",
      "泰坦尼克号\n",
      " / Titanic\n",
      "千与千寻\n",
      " / 千と千尋の神隠し\n",
      "这个杀手不太冷\n",
      " / Léon\n",
      "美丽人生\n",
      " / La vita è bella\n",
      "星际穿越\n",
      " / Interstellar\n",
      "盗梦空间\n",
      " / Inception\n",
      "楚门的世界\n",
      " / The Truman Show\n",
      "辛德勒的名单\n",
      " / Schindler's List\n",
      "忠犬八公的故事\n",
      " / Hachi: A Dog's Tale\n",
      "海上钢琴师\n",
      " / La leggenda del pianista sull'oceano\n",
      "三傻大闹宝莱坞\n",
      " / 3 Idiots\n",
      "放牛班的春天\n",
      " / Les choristes\n",
      "机器人总动员\n",
      " / WALL·E\n",
      "疯狂动物城\n",
      " / Zootopia\n",
      "无间道\n",
      " / 無間道\n",
      "控方证人\n",
      " / Witness for the Prosecution\n",
      "大话西游之大圣娶亲\n",
      " / 西遊記大結局之仙履奇緣\n",
      "熔炉\n",
      " / 도가니\n",
      "教父\n",
      " / The Godfather\n",
      "触不可及\n",
      " / Intouchables\n",
      "当幸福来敲门\n",
      " / The Pursuit of Happyness\n",
      "寻梦环游记\n",
      " / Coco\n",
      "Movie title list printed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36\"\n",
    "}\n",
    "resp = requests.get(url, headers=headers)\n",
    "page_content = resp.text\n",
    "\n",
    "# Parse the page content with BeautifulSoup\n",
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "# Find movie titles using CSS selector\n",
    "movie_titles = soup.select(\"#content > div > div.article > ol > li > div > div.info > div.hd > a > span.title\")\n",
    "\n",
    "# Print the movie titles\n",
    "for title in movie_titles:\n",
    "    print(title.get_text())\n",
    "\n",
    "print(\"Movie title list printed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1eb68bb-6490-4512-acf7-ee63d916a784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: zgghaapkfhy.jpg\n",
      "Downloaded: zyecmylhfrn.jpg\n",
      "Downloaded: gsyxb1o4gdq.jpg\n",
      "Downloaded: zgghaapkfhy.jpg\n",
      "Downloaded: ihk3g03psgi.jpg\n",
      "Downloaded: t1ouhdmbhjo.jpg\n",
      "Downloaded: ap2c1vg3whm.jpg\n",
      "Downloaded: zj2ggdrhl44.jpg\n",
      "Downloaded: dnvk3qz2ocy.jpg\n",
      "Downloaded: cyhlqhlylep.jpg\n",
      "Downloaded: ql23ngdggqt.jpg\n",
      "Downloaded: fhkfzrkfyyv.jpg\n",
      "Downloaded: vxrtmf3rnig.jpg\n",
      "Downloaded: xbz4cl1lhtg.jpg\n",
      "Downloaded: yotyomy0svb.jpg\n",
      "Downloaded: 5g54nolova5.jpg\n",
      "Downloaded: y1mahuysmqw.jpg\n",
      "Downloaded: u0ffxygdpgk.jpg\n",
      "Downloaded: epb4dxkxtlz.jpg\n",
      "Downloaded: f3ypjikdmf0.jpg\n",
      "Downloaded: ghj2jfe5twm.jpg\n",
      "Downloaded: oxxnb3niz1h.jpg\n",
      "Downloaded: c3td3px1qvo.jpg\n",
      "Downloaded: nbuidh0n0cj.jpg\n",
      "Downloaded: af2f41cry2n.jpg\n",
      "Downloaded: hv1yg315qua.jpg\n",
      "Downloaded: hpfrstvqizi.jpg\n",
      "Downloaded: w2yy320dp5b.jpg\n",
      "Downloaded: srpiuysntej.jpg\n",
      "Downloaded: j32saeaez3h.jpg\n",
      "All images downloaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = \"https://www.umei.cc/bizhitupian/weimeibizhi/\"\n",
    "resp = requests.get(url)\n",
    "resp.encoding = 'utf-8'  # Handle encoding issues\n",
    "\n",
    "# Pass the response content to BeautifulSoup\n",
    "main_page = BeautifulSoup(resp.text, \"html.parser\")\n",
    "items = main_page.find_all(\"div\", class_=\"item\")\n",
    "\n",
    "# Create 'imgC' directory if it doesn't exist\n",
    "os.makedirs(\"imgC\", exist_ok=True)\n",
    "\n",
    "for item in items:\n",
    "    # Find the link to the child page\n",
    "    link = item.find(\"a\", href=True)\n",
    "    href = link[\"href\"]\n",
    "    \n",
    "    # Check if the URL has a scheme\n",
    "    if not href.startswith(\"http\"):\n",
    "        href = urljoin(url, href)\n",
    "\n",
    "    # Get the content of the child page\n",
    "    child_page_resp = requests.get(href)\n",
    "    child_page_resp.encoding = 'utf-8'\n",
    "    child_page_text = child_page_resp.text\n",
    "\n",
    "    # Extract the image download URL from the child page\n",
    "    child_page = BeautifulSoup(child_page_text, \"html.parser\")\n",
    "    img = child_page.find(\"img\", class_=\"lazy\")\n",
    "    src = img[\"data-original\"]\n",
    "\n",
    "    # Check if the URL has a scheme\n",
    "    if not src.startswith(\"http\"):\n",
    "        src = urljoin(url, src)\n",
    "\n",
    "    # Download the image\n",
    "    img_resp = requests.get(src)\n",
    "    img_name = src.split(\"/\")[-1]  # Extract the image name from the URL\n",
    "\n",
    "    with open(\"imgC/\" + img_name, mode=\"wb\") as f:\n",
    "        f.write(img_resp.content)\n",
    "\n",
    "    print(\"Downloaded:\", img_name)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"All images downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd84a9-f80e-4b2c-b518-23e92cff0b57",
   "metadata": {},
   "source": [
    "4.3 Xpath library\n",
    "\n",
    "XPath is a powerful query language used for selecting elements and navigating XML and HTML documents. It allows you to traverse the structure of the document and extract specific data based on patterns and conditions.\n",
    "\n",
    "To use the XPath library in Python, you need to install the `lxml` library, which provides XPath functionality. You can install it using pip:\n",
    "\n",
    "`pip install lxml`\n",
    "\n",
    "Once installed, you can import the necessary modules to work with XPath:\n",
    "\n",
    "```python\n",
    "from lxml import etree\n",
    "```\n",
    "\n",
    "Now, let's go through the key concepts and techniques of XPath:\n",
    "\n",
    "1.  Selecting Elements: XPath expressions are used to select elements in an XML or HTML document. You can specify the elements you want to target by their tag names, attributes, or their position in the document's structure.\n",
    "\n",
    "2.  XPath Axes: Axes allow you to navigate the document relative to the current element. Common axes include `child`, `parent`, `descendant`, `ancestor`, `following-sibling`, and `preceding-sibling`. They help you select elements based on their relationship to other elements.\n",
    "\n",
    "3.  Predicates: Predicates are conditions that further refine the element selection. You can use predicates to filter elements based on their attributes, values, or positions.\n",
    "\n",
    "4.  XPath Functions: XPath provides a range of built-in functions to perform operations on elements and values. Functions like `text()`, `contains()`, `starts-with()`, `position()`, and `last()` are commonly used in XPath expressions.\n",
    "\n",
    "5.  XPath Operators: XPath supports various operators such as `|` (union), `+`, `-`, `*`, `div`, `mod`, `=`, `!=`, `<`, `>`, `<=`, `>=`, `and`, `or`, and `not`. These operators allow you to combine expressions and compare values.\n",
    "\n",
    "6.  Using XPath in Python: With the `lxml` library, you can parse an XML or HTML document using the `etree` module. Once parsed, you can use the `xpath()` method to execute XPath expressions and retrieve the matching elements or values.\n",
    "\n",
    "XPath is a versatile tool for extracting data from XML and HTML documents. It provides a precise and flexible way to navigate the document structure and target specific elements. By mastering XPath, you can efficiently extract the data you need from complex documents.\n",
    "\n",
    "Note: Although XPath is primarily designed for XML, it can also be used with HTML documents. However, HTML documents may have structural differences that could affect the accuracy and reliability of XPath expressions. In such cases, it is recommended to use libraries specifically designed for parsing HTML, such as BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d0255f74-86d2-4270-9f97-4cd556315cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: lxml in d:\\anaconda3\\lib\\site-packages (4.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37ddb170-a1a2-4a47-ba42-0fa1dc422c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the XPath Tutorial\n",
      "Learn XPath for web scraping\n",
      "Introduction\n",
      "Basic Syntax\n",
      "Expressions and Predicates\n",
      "Functions\n",
      "div\n",
      "div\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# Create an HTML document\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Welcome to the XPath Tutorial</h1>\n",
    "        <div class=\"content\">\n",
    "            <p>Learn XPath for web scraping</p>\n",
    "            <ul>\n",
    "                <li>Introduction</li>\n",
    "                <li>Basic Syntax</li>\n",
    "                <li>Expressions and Predicates</li>\n",
    "                <li>Functions</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML document\n",
    "root = etree.HTML(html_content)\n",
    "\n",
    "# Select elements using XPath\n",
    "headings = root.xpath(\"//h1\")\n",
    "for heading in headings:\n",
    "    print(heading.text)\n",
    "\n",
    "paragraph = root.xpath(\"//p\")[0]\n",
    "print(paragraph.text)\n",
    "\n",
    "list_items = root.xpath(\"//ul/li/text()\")\n",
    "for item in list_items:\n",
    "    print(item)\n",
    "\n",
    "# Use predicates to filter elements\n",
    "div = root.xpath(\"//div[@class='content']\")[0]\n",
    "print(div.tag)\n",
    "\n",
    "# Access parent and child elements\n",
    "ul = root.xpath(\"//ul\")[0]\n",
    "parent_div = ul.getparent()\n",
    "print(parent_div.tag)\n",
    "\n",
    "# Evaluate XPath expressions with namespaces (not applicable for HTML)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3edc1fb9-478c-4bd1-b769-12bb0ce369b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肖申克的救赎\n",
      "霸王别姬\n",
      "阿甘正传\n",
      "泰坦尼克号\n",
      "千与千寻\n",
      "这个杀手不太冷\n",
      "美丽人生\n",
      "星际穿越\n",
      "盗梦空间\n",
      "楚门的世界\n",
      "辛德勒的名单\n",
      "忠犬八公的故事\n",
      "海上钢琴师\n",
      "三傻大闹宝莱坞\n",
      "放牛班的春天\n",
      "机器人总动员\n",
      "疯狂动物城\n",
      "无间道\n",
      "控方证人\n",
      "大话西游之大圣娶亲\n",
      "熔炉\n",
      "教父\n",
      "触不可及\n",
      "当幸福来敲门\n",
      "寻梦环游记\n",
      "Movie list printed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "url = \"https://movie.douban.com/top250\"\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36\"\n",
    "}\n",
    "resp = requests.get(url, headers=headers)\n",
    "page_content = resp.text\n",
    "\n",
    "# Parse data\n",
    "tree = etree.HTML(page_content)\n",
    "\n",
    "# Find movie names using XPath\n",
    "movie_names = tree.xpath(\"/html/body/div[3]/div[1]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]\")\n",
    "\n",
    "# Print the movie names\n",
    "for name in movie_names:\n",
    "    print(name.text)\n",
    "\n",
    "print(\"Movie list printed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b364fd2-970a-4d13-b791-87130853998b",
   "metadata": {},
   "source": [
    "### 5.Anti-Anti-Crawling Strategies\n",
    "\n",
    "*   Common anti-crawling techniques\n",
    "*   Ways to bypass these techniques\n",
    "    Web scraping is subject to various anti-crawling techniques employed by websites to protect their data and control access. These techniques are designed to detect and prevent automated scraping activities. Understanding common anti-crawling techniques and learning ways to bypass them can help you improve the success rate and reliability of your web scraping projects. Let's explore some common anti-crawling techniques and methods to bypass them:\n",
    "\n",
    "Robots.txt: Websites often use a robots.txt file to specify which parts of their website should not be accessed by web crawlers. It is a standard method for communicating the crawling permissions to search engine bots. To bypass this, you can choose to ignore the robots.txt file and proceed with scraping the desired content. However, be cautious and respectful of website policies when doing so.\n",
    "\n",
    "User-Agent Restrictions: Websites may block requests that do not have a valid User-Agent header or have suspicious User-Agent values. To bypass this, you can set a User-Agent header in your scraping code to mimic a legitimate web browser. You can find popular User-Agent strings for various browsers and set them in your requests headers to make your scraper appear more like a regular user.\n",
    "\n",
    "Captcha Challenges: Captchas are used to differentiate between humans and bots. Websites may employ captchas to prevent automated scraping. To bypass captchas, you can use third-party services or libraries that can automatically solve captchas, such as CAPTCHA-solving APIs. These services typically require an API key and can handle the captcha challenges on your behalf.\n",
    "\n",
    "IP Blocking: Websites may block IP addresses that make too many requests within a short time frame. To bypass IP blocking, you can use rotating proxies or proxy services. Proxies allow you to make requests from different IP addresses, making it difficult for websites to track and block your scraping activities. Be sure to choose reliable and reputable proxy providers.\n",
    "\n",
    "Dynamic Website Content: Websites that heavily rely on client-side rendering using JavaScript may present challenges for scraping. To bypass this, you can use headless browsers or scraping frameworks that can render JavaScript, such as Puppeteer or Selenium. These tools simulate a real browser environment and allow you to interact with dynamically loaded content.\n",
    "\n",
    "Session Management: Websites may use cookies or sessions to track user activity and prevent scraping. To bypass session-based protections, you can maintain and manage cookies in your scraping code. You can extract cookies from initial requests and include them in subsequent requests to maintain a session with the website.\n",
    "\n",
    "Rate Limiting: Websites may implement rate limiting mechanisms to restrict the number of requests made by a single user within a given time period. To bypass rate limiting, you can introduce delays between requests or use intelligent scraping techniques like adaptive rate limiting, where you adjust the scraping speed dynamically based on the website's response times.\n",
    "\n",
    "Honeypot Traps: Websites may employ hidden links or form fields that are invisible to human users but detectable by bots. Submitting requests to these traps can lead to IP blocking or other countermeasures. To bypass honeypot traps, you can inspect the HTML structure of the web page, analyze form fields, or avoid clicking on suspicious links.\n",
    "\n",
    "It's important to note that while these methods can help bypass common anti-crawling techniques, they should be used responsibly and in compliance with the website's terms of service. It's always a good practice to respect website policies, limit the scraping rate, and avoid putting excessive load on the target website's servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4dc681-38f8-4de8-b90d-2ef2745aa762",
   "metadata": {},
   "source": [
    "#### 5.1 Asynchronous Scraping\n",
    "\n",
    "*   Understanding asynchronous crawling\n",
    "\n",
    "Asynchronous crawling, also known as asynchronous scraping or concurrent scraping, is a technique used in web scraping to improve the efficiency and speed of data extraction from multiple web pages. In traditional scraping, requests are sent and processed synchronously, which means that each request must wait for a response before the next request is made. This can lead to significant delays and reduced performance, especially when dealing with a large number of web pages.\n",
    "\n",
    "Asynchronous crawling solves this problem by allowing multiple requests to be made simultaneously and processed independently, without waiting for each response. This enables scraping scripts to take advantage of parallel processing and maximize the utilization of system resources. As a result, the overall scraping speed can be significantly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c237255-d88b-4b9b-b389-98af4b023130",
   "metadata": {},
   "source": [
    "#### 5.2 Selenium\n",
    "\n",
    "Selenium is a popular open-source library that provides a programming interface for automating web browsers. It enables developers to automate browser actions, interact with web elements, and perform various tasks on web pages. Selenium supports multiple programming languages, including Python, Java, C#, and more. In this description, we'll focus on Selenium with Python.\n",
    "\n",
    "Key features and components of the Selenium library include:\n",
    "\n",
    "WebDriver: WebDriver is the core component of Selenium that provides a programming interface to interact with web browsers. It allows you to automate browser actions such as navigating to URLs, filling forms, clicking buttons, and extracting data from web elements.\n",
    "\n",
    "Selenium WebDriver APIs: Selenium WebDriver provides APIs to interact with different browsers, including Chrome, Firefox, Safari, Edge, and more. Each browser requires a specific WebDriver, which acts as a bridge between the Selenium library and the browser.\n",
    "\n",
    "Locating Elements: Selenium provides various methods to locate elements on a web page, such as finding elements by their ID, class name, tag name, CSS selector, or XPath. These methods enable you to identify and interact with specific elements on a web page.\n",
    "\n",
    "Interacting with Elements: Selenium allows you to interact with web elements by performing actions like clicking buttons, filling forms, selecting options from dropdowns, submitting forms, or even simulating keyboard input. You can also retrieve element attributes, text, or perform other manipulations.\n",
    "\n",
    "Navigating and Manipulating Browser Windows: Selenium provides methods to handle multiple browser windows or tabs. You can switch between windows, open new windows, or close existing ones. It also allows you to control the browser's size, position, and perform scrolling operations.\n",
    "\n",
    "Advanced Interactions: Selenium supports advanced interactions with web elements, such as hovering over elements, double-clicking, dragging and dropping, and executing JavaScript code within the browser.\n",
    "\n",
    "Waiting for Elements: Selenium provides explicit and implicit wait mechanisms to handle dynamic web pages. You can wait for specific conditions to be met before performing actions, such as waiting for an element to be visible, clickable, or present on the page.\n",
    "\n",
    "Selenium is widely used for various purposes, including web scraping, automated testing, browser automation, and web application development. It offers flexibility and compatibility across different browsers and platforms, making it a versatile tool for automating browser interactions.\n",
    "\n",
    "In Python, the Selenium library can be installed using pip with the command pip install selenium. Additionally, you need to download and set up the corresponding WebDriver（https://chromedriver.chromium.org/downloads） for the browser you intend to automate.\n",
    "\n",
    "Selenium documentation, tutorials, and community resources are available on the official Selenium website (https://www.selenium.dev/). These resources provide detailed information, examples, and best practices to help you make the most out of the Selenium library for your web automation needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a15ac7d-90ed-4a6a-83da-8a9df02b05e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires jedi<0.19.0,>=0.17.2, but you have jedi 0.19.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting selenium\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/04/4d/a6e8afd65b87372e275eb612d564ec68f79195e9b7e27004a3b2cce69686/selenium-4.20.0-py3-none-any.whl (9.5 MB)\n",
      "     ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/9.5 MB 5.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.5/9.5 MB 5.2 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.1/9.5 MB 7.9 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.8/9.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.0/9.5 MB 12.7 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 3.9/9.5 MB 13.9 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.1/9.5 MB 13.9 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.2/9.5 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 6.0/9.5 MB 14.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 6.8/9.5 MB 14.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.3/9.5 MB 14.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.3/9.5 MB 14.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 7.7/9.5 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.2/9.5 MB 12.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 9.1/9.5 MB 12.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  9.5/9.5 MB 13.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 9.5/9.5 MB 12.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/17/c9/f86f89f14d52f9f2f652ce24cb2f60141a51d087db1563f3fba94ba07346/trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "     ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 467.2/467.2 kB 14.7 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e0/44/827b2a91a5816512fcaf3cc4ebc465ccd5d598c45cefa6703fcf4a79018f/attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: sortedcontainers in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/78/58/e860788190eba3bcce367f74d29c4675466ce8dddfba85f7827588416f01/wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in d:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Installing collected packages: wsproto, typing_extensions, sniffio, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.1.0\n",
      "    Uninstalling attrs-22.1.0:\n",
      "      Successfully uninstalled attrs-22.1.0\n",
      "Successfully installed attrs-23.2.0 outcome-1.3.0.post0 selenium-4.20.0 sniffio-1.3.1 trio-0.25.0 trio-websocket-0.11.1 typing_extensions-4.11.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e3f22b1-06ff-4f05-8780-d7224c22d5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=122.0.6261.112)\nStacktrace:\n\tGetHandleVerifier [0x00007FF76CE8AD02+56930]\n\t(No symbol) [0x00007FF76CDFF602]\n\t(No symbol) [0x00007FF76CCB42E5]\n\t(No symbol) [0x00007FF76CC91D4C]\n\t(No symbol) [0x00007FF76CD223F7]\n\t(No symbol) [0x00007FF76CD37891]\n\t(No symbol) [0x00007FF76CD1BA43]\n\t(No symbol) [0x00007FF76CCED438]\n\t(No symbol) [0x00007FF76CCEE4D1]\n\tGetHandleVerifier [0x00007FF76D206F8D+3711213]\n\tGetHandleVerifier [0x00007FF76D2604CD+4077101]\n\tGetHandleVerifier [0x00007FF76D25865F+4044735]\n\tGetHandleVerifier [0x00007FF76CF29736+706710]\n\t(No symbol) [0x00007FF76CE0B8DF]\n\t(No symbol) [0x00007FF76CE06AC4]\n\t(No symbol) [0x00007FF76CE06C1C]\n\t(No symbol) [0x00007FF76CDF68D4]\n\tBaseThreadInitThunk [0x00007FFA09BF257D+29]\n\tRtlUserThreadStart [0x00007FFA0A9EAA48+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     62\u001b[0m     mql \u001b[38;5;241m=\u001b[39m Jdmobile()\n\u001b[1;32m---> 63\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mmql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m手机\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n",
      "Cell \u001b[1;32mIn[143], line 57\u001b[0m, in \u001b[0;36mJdmobile.run\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_html()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_product(key)\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\n",
      "Cell \u001b[1;32mIn[143], line 27\u001b[0m, in \u001b[0;36mJdmobile.scrape_pages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscroll_down()\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_page()\n",
      "Cell \u001b[1;32mIn[143], line 36\u001b[0m, in \u001b[0;36mJdmobile.get_content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     htmll \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mHTML(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m)\n\u001b[0;32m     37\u001b[0m     items \u001b[38;5;241m=\u001b[39m htmll\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgl-i-wrap\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:448\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    441\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=122.0.6261.112)\nStacktrace:\n\tGetHandleVerifier [0x00007FF76CE8AD02+56930]\n\t(No symbol) [0x00007FF76CDFF602]\n\t(No symbol) [0x00007FF76CCB42E5]\n\t(No symbol) [0x00007FF76CC91D4C]\n\t(No symbol) [0x00007FF76CD223F7]\n\t(No symbol) [0x00007FF76CD37891]\n\t(No symbol) [0x00007FF76CD1BA43]\n\t(No symbol) [0x00007FF76CCED438]\n\t(No symbol) [0x00007FF76CCEE4D1]\n\tGetHandleVerifier [0x00007FF76D206F8D+3711213]\n\tGetHandleVerifier [0x00007FF76D2604CD+4077101]\n\tGetHandleVerifier [0x00007FF76D25865F+4044735]\n\tGetHandleVerifier [0x00007FF76CF29736+706710]\n\t(No symbol) [0x00007FF76CE0B8DF]\n\t(No symbol) [0x00007FF76CE06AC4]\n\t(No symbol) [0x00007FF76CE06C1C]\n\t(No symbol) [0x00007FF76CDF68D4]\n\tBaseThreadInitThunk [0x00007FFA09BF257D+29]\n\tRtlUserThreadStart [0x00007FFA0A9EAA48+40]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "class Jdmobile:\n",
    "    def __init__(self, pages=2):\n",
    "        self.url = 'https://www.jd.com/'\n",
    "        self.pages = pages\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.data = pd.DataFrame()\n",
    "\n",
    "    def open_html(self):\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "    def search_product(self, key):\n",
    "        self.wait.until(EC.presence_of_element_located((By.ID, 'key'))).send_keys(key)\n",
    "        self.wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'button'))).click()\n",
    "\n",
    "    def scrape_pages(self):\n",
    "        for _ in range(self.pages):\n",
    "            self.scroll_down()\n",
    "            self.get_content()\n",
    "            self.next_page()\n",
    "\n",
    "    def scroll_down(self):\n",
    "        for _ in range(2):\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            sleep(3)\n",
    "\n",
    "    def get_content(self):\n",
    "        htmll = etree.HTML(self.driver.page_source)\n",
    "        items = htmll.xpath('//div[@class=\"gl-i-wrap\"]')\n",
    "        for item in items:\n",
    "            D = {}\n",
    "            D['price'] = item.xpath('.//div[@class=\"p-price\"]/strong/i/text()')[0]\n",
    "            D['comment'] = item.xpath('.//div[@class=\"p-commit\"]/strong/a/text()')[0]\n",
    "            D['shopname'] = item.xpath('.//div[@class=\"p-shop\"]/span/a/text()')[0] if item.xpath('.//div[@class=\"p-shop\"]/span/a/text()') else 'None'\n",
    "            D['URL'] = 'https:' + item.xpath('.//div[@class=\"p-commit\"]/strong/a/@href')[0]\n",
    "            D['title'] = item.xpath('.//div[@class=\"p-name p-name-type-2\"]/a/em')[0].xpath('string(.)').strip()\n",
    "            image_url = item.xpath('.//div[@class=\"p-img\"]/a/img/@data-lazy-img')[0]\n",
    "            D['pnglink'] = 'https:' + image_url if image_url != 'done' else 'https:' + item.xpath('.//div[@class=\"p-img\"]/a/img/@src')[0]\n",
    "            self.data = pd.concat([self.data, pd.DataFrame([D])])\n",
    "\n",
    "    def next_page(self):\n",
    "        next_button = self.wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"J_bottomPage\"]/span[1]/a[9]')))\n",
    "        self.driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        sleep(4)\n",
    "\n",
    "    def run(self, key):\n",
    "        self.open_html()\n",
    "        self.search_product(key)\n",
    "        self.scrape_pages()\n",
    "        self.driver.quit()\n",
    "        return self.data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mql = Jdmobile()\n",
    "    data = mql.run('手机')\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47279fb0-9ae3-4258-8f5c-0bcf1afe1394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
